// Generated by the Scala Plugin for the Protocol Buffer Compiler.
// Do not edit!

package io.milvus.grpc.milvus

/** @param collectionName
  *   target collection
  * @param partitionName
  *   target partition
  * @param channelNames
  *   channel names for the collection
  * @param rowBased
  *   the file is row-based or column-based
  * @param files
  *   file paths to be imported
  * @param options
  *   import options, bucket, etc.
  * @param dbName
  *   target database
  * @param clusteringInfo
  *   serialized `schema.ClusteringInfo`
  */
@SerialVersionUID(0L)
final case class ImportRequest(
    collectionName: _root_.scala.Predef.String = "",
    partitionName: _root_.scala.Predef.String = "",
    channelNames: _root_.scala.Seq[_root_.scala.Predef.String] = _root_.scala.Seq.empty,
    rowBased: _root_.scala.Boolean = false,
    files: _root_.scala.Seq[_root_.scala.Predef.String] = _root_.scala.Seq.empty,
    options: _root_.scala.Seq[io.milvus.grpc.common.KeyValuePair] = _root_.scala.Seq.empty,
    dbName: _root_.scala.Predef.String = "",
    clusteringInfo: _root_.com.google.protobuf.ByteString = _root_.com.google.protobuf.ByteString.EMPTY,
    unknownFields: _root_.scalapb.UnknownFieldSet = _root_.scalapb.UnknownFieldSet.empty
    ) extends scalapb.GeneratedMessage with scalapb.lenses.Updatable[ImportRequest] {
    @transient
    private[this] var __serializedSizeMemoized: _root_.scala.Int = 0
    private[this] def __computeSerializedSize(): _root_.scala.Int = {
      var __size = 0
      
      {
        val __value = collectionName
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(1, __value)
        }
      };
      
      {
        val __value = partitionName
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(2, __value)
        }
      };
      channelNames.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(3, __value)
      }
      
      {
        val __value = rowBased
        if (__value != false) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBoolSize(4, __value)
        }
      };
      files.foreach { __item =>
        val __value = __item
        __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(5, __value)
      }
      options.foreach { __item =>
        val __value = __item
        __size += 1 + _root_.com.google.protobuf.CodedOutputStream.computeUInt32SizeNoTag(__value.serializedSize) + __value.serializedSize
      }
      
      {
        val __value = dbName
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeStringSize(7, __value)
        }
      };
      
      {
        val __value = clusteringInfo
        if (!__value.isEmpty) {
          __size += _root_.com.google.protobuf.CodedOutputStream.computeBytesSize(8, __value)
        }
      };
      __size += unknownFields.serializedSize
      __size
    }
    override def serializedSize: _root_.scala.Int = {
      var __size = __serializedSizeMemoized
      if (__size == 0) {
        __size = __computeSerializedSize() + 1
        __serializedSizeMemoized = __size
      }
      __size - 1
      
    }
    def writeTo(`_output__`: _root_.com.google.protobuf.CodedOutputStream): _root_.scala.Unit = {
      {
        val __v = collectionName
        if (!__v.isEmpty) {
          _output__.writeString(1, __v)
        }
      };
      {
        val __v = partitionName
        if (!__v.isEmpty) {
          _output__.writeString(2, __v)
        }
      };
      channelNames.foreach { __v =>
        val __m = __v
        _output__.writeString(3, __m)
      };
      {
        val __v = rowBased
        if (__v != false) {
          _output__.writeBool(4, __v)
        }
      };
      files.foreach { __v =>
        val __m = __v
        _output__.writeString(5, __m)
      };
      options.foreach { __v =>
        val __m = __v
        _output__.writeTag(6, 2)
        _output__.writeUInt32NoTag(__m.serializedSize)
        __m.writeTo(_output__)
      };
      {
        val __v = dbName
        if (!__v.isEmpty) {
          _output__.writeString(7, __v)
        }
      };
      {
        val __v = clusteringInfo
        if (!__v.isEmpty) {
          _output__.writeBytes(8, __v)
        }
      };
      unknownFields.writeTo(_output__)
    }
    def withCollectionName(__v: _root_.scala.Predef.String): ImportRequest = copy(collectionName = __v)
    def withPartitionName(__v: _root_.scala.Predef.String): ImportRequest = copy(partitionName = __v)
    def clearChannelNames = copy(channelNames = _root_.scala.Seq.empty)
    def addChannelNames(__vs: _root_.scala.Predef.String *): ImportRequest = addAllChannelNames(__vs)
    def addAllChannelNames(__vs: Iterable[_root_.scala.Predef.String]): ImportRequest = copy(channelNames = channelNames ++ __vs)
    def withChannelNames(__v: _root_.scala.Seq[_root_.scala.Predef.String]): ImportRequest = copy(channelNames = __v)
    def withRowBased(__v: _root_.scala.Boolean): ImportRequest = copy(rowBased = __v)
    def clearFiles = copy(files = _root_.scala.Seq.empty)
    def addFiles(__vs: _root_.scala.Predef.String *): ImportRequest = addAllFiles(__vs)
    def addAllFiles(__vs: Iterable[_root_.scala.Predef.String]): ImportRequest = copy(files = files ++ __vs)
    def withFiles(__v: _root_.scala.Seq[_root_.scala.Predef.String]): ImportRequest = copy(files = __v)
    def clearOptions = copy(options = _root_.scala.Seq.empty)
    def addOptions(__vs: io.milvus.grpc.common.KeyValuePair *): ImportRequest = addAllOptions(__vs)
    def addAllOptions(__vs: Iterable[io.milvus.grpc.common.KeyValuePair]): ImportRequest = copy(options = options ++ __vs)
    def withOptions(__v: _root_.scala.Seq[io.milvus.grpc.common.KeyValuePair]): ImportRequest = copy(options = __v)
    def withDbName(__v: _root_.scala.Predef.String): ImportRequest = copy(dbName = __v)
    def withClusteringInfo(__v: _root_.com.google.protobuf.ByteString): ImportRequest = copy(clusteringInfo = __v)
    def withUnknownFields(__v: _root_.scalapb.UnknownFieldSet) = copy(unknownFields = __v)
    def discardUnknownFields = copy(unknownFields = _root_.scalapb.UnknownFieldSet.empty)
    def getFieldByNumber(__fieldNumber: _root_.scala.Int): _root_.scala.Any = {
      (__fieldNumber: @_root_.scala.unchecked) match {
        case 1 => {
          val __t = collectionName
          if (__t != "") __t else null
        }
        case 2 => {
          val __t = partitionName
          if (__t != "") __t else null
        }
        case 3 => channelNames
        case 4 => {
          val __t = rowBased
          if (__t != false) __t else null
        }
        case 5 => files
        case 6 => options
        case 7 => {
          val __t = dbName
          if (__t != "") __t else null
        }
        case 8 => {
          val __t = clusteringInfo
          if (__t != _root_.com.google.protobuf.ByteString.EMPTY) __t else null
        }
      }
    }
    def getField(__field: _root_.scalapb.descriptors.FieldDescriptor): _root_.scalapb.descriptors.PValue = {
      _root_.scala.Predef.require(__field.containingMessage eq companion.scalaDescriptor)
      (__field.number: @_root_.scala.unchecked) match {
        case 1 => _root_.scalapb.descriptors.PString(collectionName)
        case 2 => _root_.scalapb.descriptors.PString(partitionName)
        case 3 => _root_.scalapb.descriptors.PRepeated(channelNames.iterator.map(_root_.scalapb.descriptors.PString(_)).toVector)
        case 4 => _root_.scalapb.descriptors.PBoolean(rowBased)
        case 5 => _root_.scalapb.descriptors.PRepeated(files.iterator.map(_root_.scalapb.descriptors.PString(_)).toVector)
        case 6 => _root_.scalapb.descriptors.PRepeated(options.iterator.map(_.toPMessage).toVector)
        case 7 => _root_.scalapb.descriptors.PString(dbName)
        case 8 => _root_.scalapb.descriptors.PByteString(clusteringInfo)
      }
    }
    def toProtoString: _root_.scala.Predef.String = _root_.scalapb.TextFormat.printToUnicodeString(this)
    def companion: io.milvus.grpc.milvus.ImportRequest.type = io.milvus.grpc.milvus.ImportRequest
    // @@protoc_insertion_point(GeneratedMessage[milvus.proto.milvus.ImportRequest])
}

object ImportRequest extends scalapb.GeneratedMessageCompanion[io.milvus.grpc.milvus.ImportRequest] {
  implicit def messageCompanion: scalapb.GeneratedMessageCompanion[io.milvus.grpc.milvus.ImportRequest] = this
  def parseFrom(`_input__`: _root_.com.google.protobuf.CodedInputStream): io.milvus.grpc.milvus.ImportRequest = {
    var __collectionName: _root_.scala.Predef.String = ""
    var __partitionName: _root_.scala.Predef.String = ""
    val __channelNames: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String]
    var __rowBased: _root_.scala.Boolean = false
    val __files: _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String] = new _root_.scala.collection.immutable.VectorBuilder[_root_.scala.Predef.String]
    val __options: _root_.scala.collection.immutable.VectorBuilder[io.milvus.grpc.common.KeyValuePair] = new _root_.scala.collection.immutable.VectorBuilder[io.milvus.grpc.common.KeyValuePair]
    var __dbName: _root_.scala.Predef.String = ""
    var __clusteringInfo: _root_.com.google.protobuf.ByteString = _root_.com.google.protobuf.ByteString.EMPTY
    var `_unknownFields__`: _root_.scalapb.UnknownFieldSet.Builder = null
    var _done__ = false
    while (!_done__) {
      val _tag__ = _input__.readTag()
      _tag__ match {
        case 0 => _done__ = true
        case 10 =>
          __collectionName = _input__.readStringRequireUtf8()
        case 18 =>
          __partitionName = _input__.readStringRequireUtf8()
        case 26 =>
          __channelNames += _input__.readStringRequireUtf8()
        case 32 =>
          __rowBased = _input__.readBool()
        case 42 =>
          __files += _input__.readStringRequireUtf8()
        case 50 =>
          __options += _root_.scalapb.LiteParser.readMessage[io.milvus.grpc.common.KeyValuePair](_input__)
        case 58 =>
          __dbName = _input__.readStringRequireUtf8()
        case 66 =>
          __clusteringInfo = _input__.readBytes()
        case tag =>
          if (_unknownFields__ == null) {
            _unknownFields__ = new _root_.scalapb.UnknownFieldSet.Builder()
          }
          _unknownFields__.parseField(tag, _input__)
      }
    }
    io.milvus.grpc.milvus.ImportRequest(
        collectionName = __collectionName,
        partitionName = __partitionName,
        channelNames = __channelNames.result(),
        rowBased = __rowBased,
        files = __files.result(),
        options = __options.result(),
        dbName = __dbName,
        clusteringInfo = __clusteringInfo,
        unknownFields = if (_unknownFields__ == null) _root_.scalapb.UnknownFieldSet.empty else _unknownFields__.result()
    )
  }
  implicit def messageReads: _root_.scalapb.descriptors.Reads[io.milvus.grpc.milvus.ImportRequest] = _root_.scalapb.descriptors.Reads{
    case _root_.scalapb.descriptors.PMessage(__fieldsMap) =>
      _root_.scala.Predef.require(__fieldsMap.keys.forall(_.containingMessage eq scalaDescriptor), "FieldDescriptor does not match message type.")
      io.milvus.grpc.milvus.ImportRequest(
        collectionName = __fieldsMap.get(scalaDescriptor.findFieldByNumber(1).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        partitionName = __fieldsMap.get(scalaDescriptor.findFieldByNumber(2).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        channelNames = __fieldsMap.get(scalaDescriptor.findFieldByNumber(3).get).map(_.as[_root_.scala.Seq[_root_.scala.Predef.String]]).getOrElse(_root_.scala.Seq.empty),
        rowBased = __fieldsMap.get(scalaDescriptor.findFieldByNumber(4).get).map(_.as[_root_.scala.Boolean]).getOrElse(false),
        files = __fieldsMap.get(scalaDescriptor.findFieldByNumber(5).get).map(_.as[_root_.scala.Seq[_root_.scala.Predef.String]]).getOrElse(_root_.scala.Seq.empty),
        options = __fieldsMap.get(scalaDescriptor.findFieldByNumber(6).get).map(_.as[_root_.scala.Seq[io.milvus.grpc.common.KeyValuePair]]).getOrElse(_root_.scala.Seq.empty),
        dbName = __fieldsMap.get(scalaDescriptor.findFieldByNumber(7).get).map(_.as[_root_.scala.Predef.String]).getOrElse(""),
        clusteringInfo = __fieldsMap.get(scalaDescriptor.findFieldByNumber(8).get).map(_.as[_root_.com.google.protobuf.ByteString]).getOrElse(_root_.com.google.protobuf.ByteString.EMPTY)
      )
    case _ => throw new RuntimeException("Expected PMessage")
  }
  def javaDescriptor: _root_.com.google.protobuf.Descriptors.Descriptor = io.milvus.grpc.milvus.MilvusProto.javaDescriptor.getMessageTypes().get(97)
  def scalaDescriptor: _root_.scalapb.descriptors.Descriptor = io.milvus.grpc.milvus.MilvusProto.scalaDescriptor.messages(97)
  def messageCompanionForFieldNumber(__number: _root_.scala.Int): _root_.scalapb.GeneratedMessageCompanion[?] = {
    var __out: _root_.scalapb.GeneratedMessageCompanion[?] = null
    (__number: @_root_.scala.unchecked) match {
      case 6 => __out = io.milvus.grpc.common.KeyValuePair
    }
    __out
  }
  lazy val nestedMessagesCompanions: Seq[_root_.scalapb.GeneratedMessageCompanion[? <: _root_.scalapb.GeneratedMessage]] = Seq.empty
  def enumCompanionForFieldNumber(__fieldNumber: _root_.scala.Int): _root_.scalapb.GeneratedEnumCompanion[?] = throw new MatchError(__fieldNumber)
  lazy val defaultInstance = io.milvus.grpc.milvus.ImportRequest(
    collectionName = "",
    partitionName = "",
    channelNames = _root_.scala.Seq.empty,
    rowBased = false,
    files = _root_.scala.Seq.empty,
    options = _root_.scala.Seq.empty,
    dbName = "",
    clusteringInfo = _root_.com.google.protobuf.ByteString.EMPTY
  )
  implicit class ImportRequestLens[UpperPB](_l: _root_.scalapb.lenses.Lens[UpperPB, io.milvus.grpc.milvus.ImportRequest]) extends _root_.scalapb.lenses.ObjectLens[UpperPB, io.milvus.grpc.milvus.ImportRequest](_l) {
    def collectionName: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.collectionName)((c_, f_) => c_.copy(collectionName = f_))
    def partitionName: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.partitionName)((c_, f_) => c_.copy(partitionName = f_))
    def channelNames: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Predef.String]] = field(_.channelNames)((c_, f_) => c_.copy(channelNames = f_))
    def rowBased: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Boolean] = field(_.rowBased)((c_, f_) => c_.copy(rowBased = f_))
    def files: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[_root_.scala.Predef.String]] = field(_.files)((c_, f_) => c_.copy(files = f_))
    def options: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Seq[io.milvus.grpc.common.KeyValuePair]] = field(_.options)((c_, f_) => c_.copy(options = f_))
    def dbName: _root_.scalapb.lenses.Lens[UpperPB, _root_.scala.Predef.String] = field(_.dbName)((c_, f_) => c_.copy(dbName = f_))
    def clusteringInfo: _root_.scalapb.lenses.Lens[UpperPB, _root_.com.google.protobuf.ByteString] = field(_.clusteringInfo)((c_, f_) => c_.copy(clusteringInfo = f_))
  }
  final val COLLECTION_NAME_FIELD_NUMBER = 1
  final val PARTITION_NAME_FIELD_NUMBER = 2
  final val CHANNEL_NAMES_FIELD_NUMBER = 3
  final val ROW_BASED_FIELD_NUMBER = 4
  final val FILES_FIELD_NUMBER = 5
  final val OPTIONS_FIELD_NUMBER = 6
  final val DB_NAME_FIELD_NUMBER = 7
  final val CLUSTERING_INFO_FIELD_NUMBER = 8
  def of(
    collectionName: _root_.scala.Predef.String,
    partitionName: _root_.scala.Predef.String,
    channelNames: _root_.scala.Seq[_root_.scala.Predef.String],
    rowBased: _root_.scala.Boolean,
    files: _root_.scala.Seq[_root_.scala.Predef.String],
    options: _root_.scala.Seq[io.milvus.grpc.common.KeyValuePair],
    dbName: _root_.scala.Predef.String,
    clusteringInfo: _root_.com.google.protobuf.ByteString
  ): _root_.io.milvus.grpc.milvus.ImportRequest = _root_.io.milvus.grpc.milvus.ImportRequest(
    collectionName,
    partitionName,
    channelNames,
    rowBased,
    files,
    options,
    dbName,
    clusteringInfo
  )
  // @@protoc_insertion_point(GeneratedMessageCompanion[milvus.proto.milvus.ImportRequest])
}
